# ============================================================
# Enterprise Agent Platform - Environment Configuration
# ============================================================
# Copy this file to .env (production) or .env.dev (local dev)
# and fill in your values.
#
# For local development:
#   cp .env.example .env.dev
#   ./scripts/dev-start.sh
#
# NEVER commit .env or .env.dev with real secrets to git.
# ============================================================

# ------------------------------------------------------------
# Application
# ------------------------------------------------------------
ENVIRONMENT=dev
# Options: dev | prod | test
# dev mode: enables debug, uses DEV_JWT_SECRET for auth, relaxes security checks

SECRET_KEY=change-me-to-a-random-256-bit-hex-string
# Generate with: python -c "import secrets; print(secrets.token_hex(32))"

DEBUG=false
# Automatically set to true when ENVIRONMENT=dev

# ------------------------------------------------------------
# Database (PostgreSQL 16 + pgvector)
# ------------------------------------------------------------
DATABASE_URL=postgresql+asyncpg://app:app_password@localhost:5432/enterprise_agents
# Docker Compose internal URL: postgresql+asyncpg://app:app_password@db:5432/enterprise_agents

POSTGRES_USER=app
POSTGRES_PASSWORD=app_password
POSTGRES_DB=enterprise_agents

DB_ECHO_SQL=false
# Set to true to log all SQL queries (very verbose, dev only)

# ------------------------------------------------------------
# Redis
# ------------------------------------------------------------
REDIS_URL=redis://localhost:6379/0
# Docker Compose internal URL: redis://redis:6379/0

# ------------------------------------------------------------
# LiteLLM Proxy
# ------------------------------------------------------------
LITELLM_BASE_URL=http://localhost:4000
LITELLM_API_KEY=sk-litellm-dev-key

# Model identifiers (LiteLLM format: provider/model-name)
LITELLM_DEFAULT_MODEL=openai/gpt-4o-mini
LITELLM_EMBEDDING_MODEL=openai/text-embedding-3-small

# For fully offline development, use the built-in mock:
#   uvicorn src.testing.mock_llm:app --port 4000
# Then set:
#   LITELLM_DEFAULT_MODEL=mock/dev-model
#   LITELLM_EMBEDDING_MODEL=mock/embed-model

# For local Ollama (no API cost):
#   LITELLM_DEFAULT_MODEL=ollama/llama3.2
#   LITELLM_EMBEDDING_MODEL=ollama/nomic-embed-text

# ------------------------------------------------------------
# Cloud LLM API Keys (optional - only for cloud providers)
# ------------------------------------------------------------
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...

# ------------------------------------------------------------
# Ollama (local LLM inference - optional)
# ------------------------------------------------------------
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3.2

# ------------------------------------------------------------
# OIDC / Auth
# ------------------------------------------------------------
OIDC_ISSUER_URL=https://your-idp.example.com
OIDC_CLIENT_ID=enterprise-agents
OIDC_AUDIENCE=enterprise-agents-api

# Dev mode only: symmetric JWT secret (skips OIDC JWKS fetch)
# Used when ENVIRONMENT=dev or ENVIRONMENT=test
DEV_JWT_SECRET=dev-only-jwt-secret-not-for-production

# ------------------------------------------------------------
# CORS
# ------------------------------------------------------------
CORS_ALLOWED_ORIGINS=["https://app.example.com","https://admin.example.com"]
# In dev mode, localhost origins are automatically added.
# In prod, set exact frontend URLs. Example:
#   CORS_ALLOWED_ORIGINS=["https://app.mycompany.com"]

# ------------------------------------------------------------
# Rate Limiting
# ------------------------------------------------------------
RATE_LIMIT_PER_MINUTE=60
# Per user per minute. Increase for dev (e.g. 1000).

# ------------------------------------------------------------
# RAG / Document Processing
# ------------------------------------------------------------
CHUNK_SIZE_TOKENS=512
CHUNK_OVERLAP_TOKENS=50
VECTOR_TOP_K=5
EMBEDDING_DIMENSIONS=1536
# Must match the output dimension of your embedding model.
# text-embedding-3-small = 1536, text-embedding-3-large = 3072

# ------------------------------------------------------------
# Model Routing & Token Economy
# ------------------------------------------------------------
MODEL_ROUTING_ENABLED=true
MODEL_LIGHT=ollama/qwen2.5:7b
MODEL_STANDARD=ollama/qwen2.5:32b
MODEL_HEAVY=vllm/qwen2.5:72b
TOKEN_BUDGET_DAILY=1000000
TOKEN_BUDGET_MONTHLY=20000000

# ------------------------------------------------------------
# Observability (optional)
# ------------------------------------------------------------
ENABLE_TELEMETRY=false
OTLP_ENDPOINT=http://localhost:4317
# LOG_LEVEL=INFO
# SENTRY_DSN=https://...
