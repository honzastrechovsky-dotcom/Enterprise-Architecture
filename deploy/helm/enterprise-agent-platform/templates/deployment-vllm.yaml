{{- if .Values.vllm.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "enterprise-agent-platform.fullname" . }}-vllm
  labels:
    {{- include "enterprise-agent-platform.labels" . | nindent 4 }}
    app.kubernetes.io/component: vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      {{- include "enterprise-agent-platform.selectorLabels" . | nindent 6 }}
      app.kubernetes.io/component: vllm
  template:
    metadata:
      labels:
        {{- include "enterprise-agent-platform.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: vllm
    spec:
      # GPU workloads typically need more permissive security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      containers:
      - name: vllm
        image: "{{ .Values.vllm.image.repository }}:{{ .Values.vllm.image.tag }}"
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        env:
        - name: MODEL_NAME
          value: {{ .Values.vllm.modelName | quote }}
        - name: TENSOR_PARALLEL_SIZE
          value: {{ .Values.vllm.gpuCount | quote }}
        command:
        - python
        - -m
        - vllm.entrypoints.openai.api_server
        - --model
        - $(MODEL_NAME)
        - --tensor-parallel-size
        - $(TENSOR_PARALLEL_SIZE)
        - --host
        - "0.0.0.0"
        - --port
        - "8000"
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 300  # Model loading takes time
          periodSeconds: 60
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
        resources:
          {{- toYaml .Values.vllm.resources | nindent 10 }}
        volumeMounts:
        - name: shm
          mountPath: /dev/shm
        - name: cache
          mountPath: /root/.cache
      volumes:
      # vLLM requires shared memory for tensor parallelism
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 16Gi
      - name: cache
        emptyDir: {}
      # GPU node affinity (adjust labels to match your cluster)
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu
                operator: Exists
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: {{ include "enterprise-agent-platform.fullname" . }}-vllm
  labels:
    {{- include "enterprise-agent-platform.labels" . | nindent 4 }}
    app.kubernetes.io/component: vllm
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: http
    protocol: TCP
    name: http
  selector:
    {{- include "enterprise-agent-platform.selectorLabels" . | nindent 4 }}
    app.kubernetes.io/component: vllm
{{- end }}
